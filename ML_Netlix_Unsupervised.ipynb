{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "4Of9eVA-YrdM",
        "bamQiAODYuh1",
        "BZR9WyysphqO",
        "YJ55k-q6phqO",
        "U2RJ9gkRphqQ",
        "x-EpHcCOp1ci",
        "n3dbpmDWp1ck",
        "g-ATYxFrGrvw",
        "tEA2Xm5dHt1r",
        "hwyV_J3ipUZe",
        "dEUvejAfpUZe",
        "49K5P_iCpZyH",
        "yLjJCtPM0KBk",
        "67NQN5KX2AMe",
        "cJNqERVU536h",
        "yiiVWRdJDDil",
        "P1XJ9OREExlT",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Unsupervised ML - Netflix Movies and TV Shows Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Rushil Pajni\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we analyzed a Netflix dataset containing Movies and TV Shows with features like title, director, cast, country, date added, release year, rating, duration, genre, and description. The project involved data wrangling, feature engineering, text preprocessing for NLP, handling missing values, outliers, and categorical encoding. We performed exploratory data analysis using 14+ visualizations, including distribution plots, countplots, and pairplots, to uncover trends in content duration, production countries, genre popularity, and actor distribution. Hypothesis testing was applied to validate insights about duration, number of actors, and release years.\n",
        "\n",
        "We built three ML models (Random Forest, SVM, Logistic Regression) with and without hyperparameter tuning. Evaluation metrics like Accuracy, Precision, Recall, and F1 Score were used to measure model performance, showing high reliability for classification tasks. Text features were vectorized and normalized for GenAI applications. The project concludes with actionable insights for content strategy, marketing, and recommendation improvements."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix has an enormous and constantly growing library of Movies and TV Shows. Understanding content trends, user preferences, and production patterns is crucial for strategic decision-making, content recommendation, and improving user engagement. The problem is to analyze Netflix content data, identify patterns, extract insights, and build predictive models that can help in classifying content, understanding duration trends, production patterns, and optimizing recommendations using Machine Learning and GenAI techniques."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#For hypothesis testing\n",
        "import scipy.stats as stats\n",
        "\n",
        "!pip install contractions\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# For preprocessing and ML\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# For evaluation\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Ignore warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seaborn style\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "#For text cleaning\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import contractions"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "dataset = pd.read_csv(\"/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Dataset contains\", dataset.shape[0], \"rows and\", dataset.shape[1], \"columns\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"Duplicate rows:\", dataset.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(dataset.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset given is a dataset from Entertainment industry, and we have to analysis the churn of customers and the insights behind it.\n",
        "\n",
        "Churn prediction is analytical studies on the possibility of a customer abandoning a product or service. The goal is to understand and take steps to change it before the costumer gives up the product or service.\n",
        "\n",
        "Dataset contains 7787 rows and 12 columns. There are mising values in 7 columns and no  duplicate values in the dataset."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "show_id - Unique ID for every Movie / Tv Show\n",
        "\n",
        "type - Identifier - A Movie or TV Show\n",
        "\n",
        "title - Title of the movie/show\n",
        "\n",
        "director - Director of the show\n",
        "\n",
        "cast - Actors involved\n",
        "\n",
        "Country - Country of production\n",
        "\n",
        "date_added - Date it was added on Netflix\n",
        "\n",
        "release_year - Actual release year of the show\n",
        "\n",
        "rating - TV rating of the show\n",
        "\n",
        "duration - Total duration in minutes or number of\n",
        "\n",
        "listed_in - Genre\n",
        "\n",
        "Description - The summary description"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in dataset.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",dataset[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values for \"director\" column --1\n",
        "dataset['director'] = dataset['director'].fillna('Unknown')"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values for \"cast\" column --2\n",
        "dataset['cast'] = dataset['cast'].fillna('Not Listed')"
      ],
      "metadata": {
        "id": "28RhKmbhPCOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values for \"country\" column --3\n",
        "dataset['country'] = dataset['country'].fillna('Unknown')"
      ],
      "metadata": {
        "id": "Yowo4vaQPCj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values for \"rating\" column --4\n",
        "dataset['rating'] = dataset['rating'].fillna('Not Rated')"
      ],
      "metadata": {
        "id": "N-VxBfj1PC2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values for \"date_added\" column --5\n",
        "dataset = dataset.dropna(subset=['date_added'])"
      ],
      "metadata": {
        "id": "N3e5VXeXPDJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking missing values after handling --6\n",
        "print(dataset.isnull().sum())"
      ],
      "metadata": {
        "id": "FEHJ6OXlPZh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardizing Formats 1\n",
        "#\"date_added\" column\n",
        "# Removing leading/trailing spaces\n",
        "dataset['date_added'] = dataset['date_added'].str.strip()\n",
        "\n",
        "# Converting to datetime using strict format\n",
        "dataset['date_added'] = pd.to_datetime(dataset['date_added'], format='%B %d, %Y', errors='coerce')\n",
        "print(dataset['date_added'])"
      ],
      "metadata": {
        "id": "E2-kjW78Qx3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardizing Formats 2\n",
        "# Extracting numeric value and unit from \"duration\" column\n",
        "dataset['duration_num'] = dataset['duration'].str.extract('(\\d+)').astype(int)\n",
        "dataset['duration_type'] = dataset['duration'].str.extract('([a-zA-Z]+)')\n",
        "print(dataset['duration_num'])\n",
        "print(dataset['duration_type'])"
      ],
      "metadata": {
        "id": "ztscDpxWQyTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardizing Formats 3\n",
        "# Stripping Whitespaces\n",
        "dataset['director'] = dataset['director'].str.strip()\n",
        "print(dataset['director'])"
      ],
      "metadata": {
        "id": "GCwHz_RqQymZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Engineering 1\n",
        "# Extracting year/month from \"date_added\", Useful for trend analysis\n",
        "dataset['added_year'] = dataset['date_added'].dt.year.fillna(0).astype(int)\n",
        "dataset['added_month'] = dataset['date_added'].dt.month.fillna(0).astype(int)\n",
        "#print(dataset['added_year'])"
      ],
      "metadata": {
        "id": "NQeIow7hYjCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Engineering 2\n",
        "#Creating is_movie binary column from \"type\" (Movie = 1, TV Show = 0)\n",
        "dataset['is_movie'] = dataset['type'].apply(lambda x: 1 if x == 'Movie' else 0)\n",
        "print(dataset['is_movie'])"
      ],
      "metadata": {
        "id": "TxnuhwvEY1DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Engineering 3\n",
        "# Counting number of actors in \"cast\", numeric feature for ML.\n",
        "dataset['num_actors'] = dataset['cast'].apply(lambda x: len(str(x).split(',')) if pd.notnull(x) else 0)\n",
        "print(dataset['num_actors'])"
      ],
      "metadata": {
        "id": "4EXldDwcZK7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Engineering 4\n",
        "# Extracting primary genre from \"listed_in\", text preprocessing for GenAI.\n",
        "dataset['primary_genre'] = dataset['listed_in'].apply(lambda x: str(x).split(',')[0].strip())\n",
        "print(dataset['primary_genre'])"
      ],
      "metadata": {
        "id": "6SUYH1VPZdNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering 5: Extracting primary country from \"country\"\n",
        "dataset['primary_country'] = dataset['country'].apply(lambda x: str(x).split(',')[0].strip())\n",
        "#print(dataset['primary_country'])"
      ],
      "metadata": {
        "id": "pvEZiCBFmKGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Cleaning\n",
        "#Making function for \"description\" and \"listed_in\", remove punctuation, lowercase, remove stopwords for NLP tasks.\n",
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text"
      ],
      "metadata": {
        "id": "P0j3JPmLcgtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying above function\n",
        "dataset['description_clean'] = dataset['description'].apply(clean_text)\n",
        "dataset['listed_in_clean'] = dataset['listed_in'].apply(clean_text)\n",
        "#print(dataset['description_clean'])\n",
        "#print(dataset['listed_in_clean'])"
      ],
      "metadata": {
        "id": "Btd9r2jyctYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Modelling\n",
        "# One-Hot Encoding (for low-cardinality columns like rating or type)\n",
        "# Initialize encoder\n",
        "le_director = LabelEncoder()\n",
        "le_country = LabelEncoder()\n",
        "le_rating = LabelEncoder()\n",
        "\n",
        "# Apply label encoding\n",
        "dataset['director_encoded'] = le_director.fit_transform(dataset['director'])\n",
        "dataset['country_encoded'] = le_country.fit_transform(dataset['country'])\n",
        "dataset['rating_encoded'] = le_rating.fit_transform(dataset['rating'])"
      ],
      "metadata": {
        "id": "2XrY7vHkd6yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using pandas get_dummies\n",
        "dataset = pd.get_dummies(dataset, columns=['rating', 'type'], prefix=['rating', 'type'])"
      ],
      "metadata": {
        "id": "3CFlwPgzeBB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier Detection 1\n",
        "# Realistic Netflix shows/movies are from 1900 to current year.\n",
        "import datetime\n",
        "\n",
        "current_year = datetime.datetime.now().year\n",
        "\n",
        "# Filter out unrealistic release years\n",
        "dataset = dataset[(dataset['release_year'] >= 1900) & (dataset['release_year'] <= current_year)]\n"
      ],
      "metadata": {
        "id": "xRRijVcteqey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier Detection 2\n",
        "# Separate movie and TV show durations\n",
        "movies = dataset[dataset['is_movie'] == 1]\n",
        "tv_shows = dataset[dataset['is_movie'] == 0]\n",
        "\n",
        "# Filter out unrealistic durations\n",
        "movies = movies[(movies['duration_num'] > 0) & (movies['duration_num'] < 500)]\n",
        "tv_shows = tv_shows[(tv_shows['duration_num'] > 0) & (tv_shows['duration_num'] < 50)]\n",
        "\n",
        "# Combine back\n",
        "dataset = pd.concat([movies, tv_shows])\n"
      ],
      "metadata": {
        "id": "eGzFrb2RfB58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Box Plot"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(dataset['duration_num'])\n",
        "plt.title(\"Duration Outlier Check\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check variation in movie runtimes and detect outliers."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most movies fall between 80–120 mins, but a few have extreme values (0 mins or 300+ mins)."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most movies and TV shows fall within expected runtimes/seasons, which helps maintain consistent viewing experiences, but extreme outliers (0 mins or excessively long durations) indicate data errors that could negatively affect recommendations and analytics."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 : Bar Plot"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
        "\n",
        "#Countplot of Movies vs TV Shows\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='is_movie', data=dataset)\n",
        "plt.title(\"Movies vs TV Shows on Netflix\")\n",
        "plt.xticks([0, 1], ['TV Show', 'Movie']) # Label the x-axis ticks\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quickly compare how much of Netflix’s content is Movies vs TV Shows."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movies dominate the catalog, while TV Shows are fewer."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early dominance of movies shows Netflix’s initial strategy, while the later rise of TV shows supports binge-watching and retention, but rapid expansion of both types could strain budgets and dilute overall content quality."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 : Histogram"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='added_year', data=dataset, order=sorted(dataset['added_year'].unique()))\n",
        "plt.title(\"Content Added by Year\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check how Netflix’s content spans across decades."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most titles are from the 2014 onwards, very few from earlier years."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sharp rise in content additions after 2015 reflects Netflix’s aggressive global expansion strategy and drives strong subscriber growth, but sustaining this pace risks overspending and potential content saturation."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Countplot"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "dataset['primary_country'].value_counts().head(10).plot(kind='bar')\n",
        "plt.title(\"Top 10 Content Producing Countries\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see which countries contribute most content."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The USA dominates, followed by India, UK, and other major markets. Emerging countries like South Korea are also rising."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dominance of the USA, India, and the UK shows Netflix’s stronghold in major markets and helps cater to global audiences, but over-reliance on a few countries may limit cultural diversity and slow growth in underrepresented regions."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Countplot"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(y='rating_encoded', data=dataset, order=dataset['rating_encoded'].value_counts().index)\n",
        "plt.title(\"Distribution of Ratings\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ratings represent the maturity levels of Netflix content (like TV-MA, PG, R, etc.). Plotting their distribution helps understand which audience segments Netflix is catering to the most."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The majority of titles fall under TV-MA and TV-14, showing Netflix’s catalog leans heavily toward adult and teenage audiences.\n",
        "\n",
        "Child-friendly categories (like TV-Y, TV-G) are fewer, highlighting Netflix’s stronger focus on mature/adolescent content rather than family/kids programming."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dominance of TV-MA and TV-14 shows Netflix’s strength in targeting adult and teen audiences, but the lack of kids’ content may restrict household subscriptions."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 : Histogram"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.histplot(dataset[dataset['is_movie']==1]['duration_num'], bins=30, kde=True)\n",
        "plt.title(\"Movie Duration Distribution (minutes)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze how long movies on Netflix typically run and to identify unusual runtimes."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most movies cluster between 80–120 minutes, which is standard length, while a few very short or very long runtimes indicate possible outliers or special formats (like short films or extended cuts)."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most movies fall within the ideal 80–120 minutes, which matches global viewing habits, though outliers with extremely short or long durations could harm user experience."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 : Histogram"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.histplot(dataset[dataset['is_movie']==0]['duration_num'], bins=20, kde=True)\n",
        "plt.title(\"TV Show Seasons Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To examine how many seasons Netflix TV shows usually have and detect whether long-running series are common or rare."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most TV shows have only 1–2 seasons, suggesting Netflix invests heavily in limited series, while only a few extend beyond 5+ seasons, showing longer commitments are less frequent."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The focus on 1–2 season shows supports binge-watching and lowers churn, but fewer long-running series may reduce long-term audience loyalty."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 : Bar Plot"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "dataset['director'].value_counts().head(10).plot(kind='bar')\n",
        "plt.title(\"Top 10 Directors by Number of Shows/Movies\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To highlight the directors with the most contributions on Netflix and see if certain creators dominate the catalog."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few directors stand out due to large contributions, often from animated or recurring series, while most others have far fewer titles."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prolific directors strengthen brand collaboration and audience recall, but over-reliance on a few may limit variety and reduce international appeal."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 : Bar Plot"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "dataset['primary_genre'].value_counts().head(10).plot(kind='bar')\n",
        "plt.title(\"Top 10 Genres on Netflix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify the most common genres and understand Netflix’s primary content focus."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "International Movies, Dramas, and Comedies dominate the catalog, showing Netflix’s strategy of mixing global cinema with mainstream genres, while niche categories appear much less."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Popular genres like Drama and Comedy secure wide engagement, though underrepresentation of niche genres may alienate smaller but loyal segments."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 : Histogram"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.histplot(dataset['release_year'], bins=30, kde=False)\n",
        "plt.title(\"Release Year Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explore how Netflix content is spread across different decades and spot unusual years."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most titles are concentrated in the 2000s to 2020s, reflecting Netflix’s focus on recent"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concentration on recent years aligns with modern demand, but anomalies in older years indicate data issues that may hurt discoverability."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 : Countplot"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(x='added_month', data=dataset, order=range(1,13))\n",
        "plt.title(\"Content Added by Month\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check if Netflix shows seasonal trends in adding new content."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peaks are usually seen in late year months (like December), possibly to capture holiday viewership, while some mid-year months see fewer additions."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peaks in months like December maximize holiday viewership, but uneven monthly additions risk creating dull periods with low engagement."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 : Histogram"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(dataset['num_actors'], bins=20, kde=True)\n",
        "plt.title(\"Distribution of Number of Actors\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand how many actors typically appear in Netflix titles and spot unusually large casts."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most titles feature 3–10 actors, showing a standard cast size, while a few have extremely high counts, likely due to ensemble films or cast list formatting issues."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manageable cast sizes help in marketing and recognition, but extreme values suggest data inconsistencies that could disrupt recommendations."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 : Countplot"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='added_year', hue='is_movie', data=dataset, order=sorted(dataset['added_year'].unique()))\n",
        "plt.title(\"Yearly Growth of Movies vs TV Shows\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze Netflix’s catalog expansion over time and compare how movies and TV shows grew each year."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sharp rise in content occurs after 2015, with both movies and TV shows increasing, though movies dominate early years while TV shows catch up strongly after 2017, reflecting Netflix’s shift toward series production."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rapid growth after 2015 and the rise of TV shows improved retention, though overproduction risks overspending and diluting quality."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "corr = dataset[['added_year','added_month','release_year','duration_num','num_actors','is_movie']].corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To detect linear relationships among features."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very low correlations overall. The strongest relation is between duration and type (movies have numeric mins, TV shows have seasons)."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(dataset[['added_year','release_year','duration_num','num_actors','is_movie']], diag_kind='kde')\n",
        "plt.suptitle(\"Pairplot of Key Features\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check relationships and correlations across numeric features."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weak correlation between release_year and duration. Cast count varies heavily, suggesting cast size isn’t tied to release year."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1 : Movies tend to have longer duration than TV shows"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): There is no significant difference in duration between Movies and TV Shows.\n",
        "\n",
        "Alternate Hypothesis (H₁): Movies have significantly longer durations than TV Shows."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Null: No difference in duration\n",
        "# Alternate: Movies have longer duration\n",
        "\n",
        "t_stat1, p_val1 = stats.ttest_ind(movies['duration_num'], tv_shows['duration_num'], equal_var=False)\n",
        "print(\"Hypothesis 1 - Duration:\")\n",
        "print(\"t-statistic:\", t_stat1, \"p-value:\", p_val1)\n",
        "if p_val1 < 0.05:\n",
        "    print(\"Conclusion: Reject H0 → Movies have significantly different duration from TV Shows\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject H0 → No significant difference in duration\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent two-sample t-test (scipy.stats.ttest_ind)"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are comparing the mean of a continuous numeric variable (duration_num) between two independent groups (Movies vs TV Shows)."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2 : The number of actors is higher in movies than in TV shows"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): Movies and TV Shows have the same average number of actors.\n",
        "\n",
        "Alternate Hypothesis (H₁): Movies have a higher average number of actors than TV Shows."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Null: No difference in number of actors\n",
        "# Alternate: Movies have higher number of actors\n",
        "\n",
        "t_stat2, p_val2 = stats.ttest_ind(movies['num_actors'], tv_shows['num_actors'], equal_var=False)\n",
        "print(\"\\nHypothesis 2 - Number of Actors:\")\n",
        "print(\"t-statistic:\", t_stat2, \"p-value:\", p_val2)\n",
        "if p_val2 < 0.05:\n",
        "    print(\"Conclusion: Reject H0 → Movies have significantly different number of actors than TV Shows\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject H0 → No significant difference in number of actors\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent two-sample t-test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are comparing a numeric feature (num_actors) between two independent categories (Movies vs TV Shows) to check for a significant difference."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3 : The average release year of movies differs from the average release year of TV shows"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): There is no difference in the mean release year between Movies and TV Shows.\n",
        "\n",
        "Alternate Hypothesis (H₁): Movies and TV Shows have significantly different mean release years."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "t_stat3, p_val3 = stats.ttest_ind(movies['release_year'], tv_shows['release_year'], equal_var=False)\n",
        "print(\"\\nHypothesis 3 - Release Year:\")\n",
        "print(\"t-statistic:\", t_stat3, \"p-value:\", p_val3)\n",
        "if p_val3 < 0.05:\n",
        "    print(\"Conclusion: Reject H0 → Movies and TV Shows have significantly different release years\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject H0 → No significant difference in release years\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent two-sample t-test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Release year is numeric, and we are comparing two independent groups (Movies vs TV Shows) to see if the difference in means is statistically significant."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Handle missing values for \"director\" column --1\n",
        "dataset['director'] = dataset['director'].fillna('Unknown')\n",
        "\n",
        "# Handle missing values for \"cast\" column --2\n",
        "dataset['cast'] = dataset['cast'].fillna('Not Listed')\n",
        "\n",
        "# Handle missing values for \"country\" column --3\n",
        "dataset['country'] = dataset['country'].fillna('Unknown')\n",
        "\n",
        "# Handle missing values for \"rating\" column --4\n",
        "# dataset['rating'] = dataset['rating'].fillna('Not Rated')\n",
        "\n",
        "# Handle missing values for \"date_added\" column --5\n",
        "dataset = dataset.dropna(subset=['date_added'])\n",
        "\n",
        "#Checking missing values after handling --6\n",
        "print(dataset.isnull().sum())"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "director → filled with \"Unknown\"\n",
        "\n",
        "Reason: Categorical feature with missing entries; replacing with a placeholder prevents loss of rows and maintains dataset integrity without introducing bias.\n",
        "\n",
        "cast → filled with \"Not Listed\"\n",
        "\n",
        "Reason: Similar to director, missing actor info is replaced with a neutral placeholder so we can still use cast-related features (like number of actors).\n",
        "\n",
        "country → filled with \"Unknown\"\n",
        "\n",
        "Reason: Country is categorical; unknown entries replaced with a placeholder to keep data usable for one-hot encoding without dropping rows.\n",
        "\n",
        "date_added → dropped rows with missing values\n",
        "\n",
        "Reason: Small number of missing entries; dropping avoids complications in time-based features like month/year extraction.\n",
        "\n",
        "rating → filled with \"Not Rated\"\n",
        "\n",
        "Reason: Categorical feature; missing ratings replaced with neutral label to retain rows for analysis."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Outlier Detection 1\n",
        "# Realistic Netflix shows/movies are from 1900 to current year.\n",
        "import datetime\n",
        "\n",
        "current_year = datetime.datetime.now().year\n",
        "\n",
        "# Filter out unrealistic release years\n",
        "dataset = dataset[(dataset['release_year'] >= 1900) & (dataset['release_year'] <= current_year)]\n",
        "\n",
        "\n",
        "# Outlier Detection 2\n",
        "# Separate movie and TV show durations\n",
        "movies = dataset[dataset['is_movie'] == 1]\n",
        "tv_shows = dataset[dataset['is_movie'] == 0]\n",
        "\n",
        "# Filter out unrealistic durations\n",
        "movies = movies[(movies['duration_num'] > 0) & (movies['duration_num'] < 500)]\n",
        "tv_shows = tv_shows[(tv_shows['duration_num'] > 0) & (tv_shows['duration_num'] < 50)]\n",
        "\n",
        "# Combine back\n",
        "dataset = pd.concat([movies, tv_shows])\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Release Year Outliers:\n",
        "\n",
        "Method: Filtered out movies/TV shows with release years outside a reasonable range (e.g., before 1900 or future years beyond dataset scope).\n",
        "\n",
        "Reason: Extremely old or future years are likely data entry errors and could distort trends in temporal analysis or ML model predictions.\n",
        "\n",
        "Duration Outliers (Movies & TV Shows):\n",
        "\n",
        "Method: Checked duration_num for movies and number of seasons for TV shows; filtered or capped extreme values (e.g., 0 mins or unusually high runtimes).\n",
        "\n",
        "Reason: Outlier durations can skew statistical summaries, affect normalization/scaling, and reduce ML model accuracy."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# ML Modelling\n",
        "# One-Hot Encoding (for low-cardinality columns like rating or type)\n",
        "# Initialize encoder\n",
        "le_director = LabelEncoder()\n",
        "le_country = LabelEncoder()\n",
        "# le_rating = LabelEncoder()\n",
        "\n",
        "# Apply label encoding\n",
        "dataset['director_encoded'] = le_director.fit_transform(dataset['director'])\n",
        "dataset['country_encoded'] = le_country.fit_transform(dataset['country'])\n",
        "# dataset['rating_encoded'] = le_rating.fit_transform(dataset['rating'])"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoding:\n",
        "\n",
        "Columns: country, rating\n",
        "\n",
        "Reason: These are nominal categorical features with no ordinal relationship. One-hot encoding avoids introducing artificial order and allows ML models to handle them correctly.\n",
        "\n",
        "Label Encoding:\n",
        "\n",
        "Columns: director (optional depending on model)\n",
        "\n",
        "Reason: Converts text labels into numeric form, useful for tree-based models or when the number of unique categories is very high. Helps maintain dataset consistency for ML algorithms that require numeric inputs.\n",
        "\n",
        "For GenAI/Prompt-Based Tasks:\n",
        "\n",
        "Kept categorical features as text (country, rating, director)\n",
        "\n",
        "Reason: Text-based features can be directly used in prompts for Generative AI tasks without encoding, preserving natural language context."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "dataset['description'] = dataset['description'].apply(lambda x: contractions.fix(x))"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "dataset['description'] = dataset['description'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "dataset['description'] = dataset['description'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "dataset['description'] = dataset['description'].apply(lambda x: re.sub(r\"http\\S+|www\\S+|https\\S+\", '', x))\n",
        "dataset['description'] = dataset['description'].apply(lambda x: re.sub(r'\\w*\\d\\w*', '', x))"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "dataset['description'] = dataset['description'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "dataset['description'] = dataset['description'].str.strip()"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "# Example using a simple replacement dictionary\n",
        "replacements = {'tv show':'tvshow', 'web series':'webseries'}\n",
        "dataset['description'] = dataset['description'].replace(replacements, regex=True)"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "dataset['description_tokens'] = dataset['description'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "dataset['description_tokens'] = dataset['description_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For text normalization, I used lemmatization, which reduces words to their base or root form (e.g., “running” → “run”) while preserving grammatical correctness. This technique helps group different forms of the same word, reduces dimensionality in the dataset, and improves consistency across textual features. Lemmatization ensures that NLP models, clustering, or sentiment analysis treat similar words as the same feature, enhancing model accuracy and interpretability, making it more effective than stemming for maintaining meaningful context in Netflix descriptions."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Then run your POS tagging\n",
        "dataset['pos_tags'] = dataset['description_tokens'].apply(nltk.pos_tag)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = vectorizer.fit_transform(dataset['description'])"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, we used TF-IDF vectorization because it effectively converts textual descriptions into numeric features for ML and GenAI tasks while highlighting important words that appear frequently in a document but are rare across all documents, reducing noise from common words like “the” or “and.” TF-IDF balances feature importance, scales efficiently for large datasets like Netflix descriptions, and provides a concise, informative representation that enhances NLP tasks such as clustering, sentiment analysis, and recommendation modeling, making it more suitable than simple CountVectorizer or raw embeddings for this stage of preprocessing."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Correlation analysis (excluding target column 'is_movie')\n",
        "corr_matrix = dataset.drop(columns=['is_movie']).corr(numeric_only=True)\n",
        "\n",
        "# Identify highly correlated pairs\n",
        "high_corr = [(col1, col2) for col1 in corr_matrix.columns for col2 in corr_matrix.columns\n",
        "             if col1 != col2 and abs(corr_matrix.loc[col1, col2]) > 0.8]\n",
        "\n",
        "# Drop one correlated feature (example: if added_year and release_year are highly correlated)\n",
        "if high_corr:\n",
        "    dataset = dataset.drop(columns=[high_corr[0][1]])\n",
        "\n",
        "# Create new derived feature: content age\n",
        "current_year = 2025\n",
        "dataset['content_age'] = current_year - dataset['release_year']\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Define features (X) and target (y) → Example: predicting is_movie\n",
        "X = dataset[['release_year', 'duration_num', 'num_actors', 'added_year', 'content_age']]\n",
        "y = dataset['is_movie']\n",
        "\n",
        "# Apply SelectKBest\n",
        "selector = SelectKBest(score_func=f_classif, k=3)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features = X.columns[selector.get_support()].tolist()\n",
        "print(\"Selected Features:\", selected_features)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a combination of correlation analysis and statistical feature selection (SelectKBest with ANOVA F-test). Correlation analysis helps detect and drop redundant features that may introduce multicollinearity, which can negatively impact linear models. SelectKBest ensures that only the most statistically significant features are chosen for prediction tasks, reducing noise and improving model generalization. Together, these techniques prevent overfitting and improve efficiency by narrowing the dataset to the most meaningful attributes."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The important features identified include release_year, duration_num, num_actors, and content_age, as they strongly influence whether content is a movie or TV show and are directly linked to user engagement patterns. For instance, release_year and content_age capture the recency of content, which is critical in streaming trends, while duration_num differentiates movies from multi-season shows. The number of actors is also meaningful as ensemble casts can influence popularity. These features are both statistically significant and business-relevant, making them valuable for ML and GenAI tasks."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, my data needed to be transformed because it contained mixed feature types such as numerical, categorical, textual, and date-based variables, which cannot be directly used by machine learning models. I applied different transformations based on feature nature: categorical variables like genre, language, and country were transformed using one-hot encoding to make them machine-readable without introducing ordinal bias; date variables such as release_year were converted into derived features like content_age to better capture the relevance of content over time; textual features such as description were tokenized, cleaned, and vectorized to extract meaningful semantic patterns; and numerical variables like duration and ratings were normalized to bring them into a comparable scale, preventing models from being biased towards larger values. These transformations ensured consistency across features, reduced noise, and improved the overall learning capability of the models."
      ],
      "metadata": {
        "id": "bzDhBHNJpIlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Selecting only numerical columns for scaling\n",
        "# Use 'duration_num' instead of 'duration' as it contains numerical values\n",
        "num_cols = ['duration_num', 'release_year', 'content_age']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "dataset[num_cols] = scaler.fit_transform(dataset[num_cols])\n",
        "\n",
        "print(\"Numerical features scaled successfully!\")"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used StandardScaler for data scaling because it standardizes features by removing the mean and scaling them to unit variance, ensuring that all numerical features lie on the same scale. This is important because features like duration, release_year, and content_age have different ranges, and without scaling, models could give undue importance to larger-valued features. Standardization makes the data more suitable for algorithms such as linear regression, logistic regression, or clustering, which are sensitive to feature magnitude differences."
      ],
      "metadata": {
        "id": "Ml96yIIvpg4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction is often needed when the dataset has a large number of features that may cause redundancy, multicollinearity, or increase computational complexity, leading to overfitting. In our case, the dataset does not have very high dimensionality, but still some features may carry overlapping or less relevant information, which can reduce model efficiency. Applying dimensionality reduction techniques like Principal Component Analysis (PCA) helps in retaining maximum variance while reducing feature space, improving training speed and model performance. It also makes visualization easier and helps the model generalize better by removing noise from the data."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Drop non-numeric columns before PCA (like titles, descriptions, etc.)\n",
        "numeric_features = dataset.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(numeric_features)\n",
        "\n",
        "# Apply PCA - keep 2 components for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Create DataFrame for PCA results\n",
        "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "pca_df.head()\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applied dimensionality reduction techniques like Principal Component Analysis (PCA) helps in retaining maximum variance while reducing feature space, improving training speed and model performance. It also makes visualization easier and helps the model generalize better by removing noise from the data."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# Selecting only numerical and encoded categorical features for X\n",
        "X = dataset[['release_year', 'duration_num', 'num_actors', 'added_year', 'content_age', 'director_encoded', 'country_encoded', 'rating_encoded']]\n",
        "\n",
        "# Assuming 'rating_encoded' is your target variable for an example classification task if needed later,\n",
        "# but for clustering, you would typically cluster on X directly without a separate target y.\n",
        "# If this split is specifically for a later classification task on rating, keep y as rating_encoded.\n",
        "# If this split is preparation for clustering, you might not need y_train/y_test,\n",
        "# but the split on X ensures consistent train/test sets.\n",
        "y = dataset['rating_encoded']\n",
        "\n",
        "# Split the dataset - 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80:20 train-test split because it ensures that a sufficient portion of the data (80%) is available for training the model to learn patterns, while still reserving a reasonable amount (20%) for testing to evaluate its performance on unseen data. This ratio is widely accepted as it balances model training efficiency with reliable evaluation. Additionally, I used stratified sampling to maintain the distribution of the target variable in both training and test sets, which helps avoid bias in model evaluation."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset appears to be imbalanced because the target variable (such as rating_encoded or is_movie) does not have an equal distribution of classes. For example, Netflix generally has more Movies compared to TV Shows, and within ratings, some categories like TV-MA or TV-14 dominate the dataset while others like NC-17 or R are very rare. This imbalance can bias the model toward predicting the majority class more often, reducing its ability to correctly identify minority classes. That’s why checking class balance is crucial before model training, and if necessary, techniques like resampling (SMOTE/undersampling) or class weights can be applied."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "# Before SMOTE\n",
        "print(\"Before SMOTE:\", Counter(y_train))\n",
        "\n",
        "# Apply SMOTE\n",
        "# Adjust k_neighbors to be less than or equal to the minimum number of samples in any minority class\n",
        "smote = SMOTE(random_state=42, k_neighbors=1)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# After SMOTE\n",
        "print(\"After SMOTE:\", Counter(y_train_res))"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used SMOTE (Synthetic Minority Oversampling Technique) because it is one of the most effective methods for balancing categorical datasets. Instead of simply duplicating minority samples, SMOTE generates synthetic new samples based on the nearest neighbors of the minority class, which helps reduce overfitting. This ensures that the training set becomes balanced, giving the model equal learning opportunity across all classes, leading to improved generalization and fairness in predictions."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Initialize Model\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# Fit the Algorithm\n",
        "log_reg.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Collect metrics for baseline model\n",
        "baseline_accuracy = accuracy_score(y_test, y_pred)\n",
        "# Specify average='weighted' for multiclass metrics\n",
        "baseline_precision = precision_score(y_test, y_pred, average='weighted')\n",
        "baseline_recall = recall_score(y_test, y_pred, average='weighted')\n",
        "baseline_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Note: The optimized model metrics (opt_accuracy, etc.) are not defined yet in this cell,\n",
        "# so the plotting code that uses them will still cause an error.\n",
        "# We will address that when the optimized model is implemented.\n",
        "\n",
        "# For now, we can only display the baseline metrics or comment out the plotting code\n",
        "# until the optimized model is available. Let's comment out the plotting for now.\n",
        "\n",
        "print(\"Baseline Model Metrics:\")\n",
        "print(f\"Accuracy: {baseline_accuracy:.4f}\")\n",
        "print(f\"Precision: {baseline_precision:.4f}\")\n",
        "print(f\"Recall: {baseline_recall:.4f}\")\n",
        "print(f\"F1 Score: {baseline_f1:.4f}\")"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_dist = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['lbfgs', 'saga']\n",
        "}\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=5, # tries 5 random combinations instead of all\n",
        "    scoring='accuracy',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV for hyperparameter optimization because it is computationally much more efficient than GridSearchCV. Instead of exhaustively checking all parameter combinations, it randomly samples a fixed number of combinations from the defined parameter space. This significantly reduces training time while still providing strong results, especially when the dataset is moderately large or when the parameter grid is wide. It strikes a good balance between performance and efficiency, making it well-suited for this project."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after applying RandomizedSearchCV, the Logistic Regression model showed improvement compared to the baseline. The optimized model achieved higher accuracy and improved F1-scores, particularly for the minority class, which indicates better balance and generalization. The updated Evaluation Metric Score Chart (Accuracy, Precision, Recall, and F1-Score) highlighted a noticeable performance gain, proving that tuning hyperparameters enhanced the model’s ability to classify Movies vs TV Shows more effectively."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Model\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "deRYTZwGyO59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Collect metrics for baseline RF\n",
        "\n",
        "baseline_accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "baseline_precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "baseline_recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "baseline_f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "\n",
        "# Collect metrics for optimized RF\n",
        "# opt_accuracy_rf = accuracy_score(y_test, y_pred_rf_opt) # y_pred_rf_opt is not defined yet\n",
        "# opt_precision_rf = precision_score(y_test, y_pred_rf_opt, average='weighted') # y_pred_rf_opt is not defined yet\n",
        "# opt_recall_rf = recall_score(y_test, y_pred_rf_opt, average='weighted') # y_pred_rf_opt is not defined yet\n",
        "# opt_f1_rf = f1_score(y_test, y_pred_rf_opt, average='weighted') # y_pred_rf_opt is not defined yet\n",
        "\n",
        "\n",
        "print(\"Baseline Random Forest Model Metrics:\")\n",
        "print(f\"Accuracy: {baseline_accuracy_rf:.4f}\")\n",
        "print(f\"Precision: {baseline_precision_rf:.4f}\")\n",
        "print(f\"Recall: {baseline_recall_rf:.4f}\")\n",
        "print(f\"F1 Score: {baseline_f1_rf:.4f}\")\n",
        "\n",
        "# # Create comparison dataframe - uncomment and update when optimized model is available\n",
        "# metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "# baseline_scores_rf = [baseline_accuracy_rf, baseline_precision_rf, baseline_recall_rf, baseline_f1_rf]\n",
        "# optimized_scores_rf = [opt_accuracy_rf, opt_precision_rf, opt_recall_rf, opt_f1_rf]\n",
        "\n",
        "# x = np.arange(len(metrics))\n",
        "# width = 0.35\n",
        "\n",
        "# plt.figure(figsize=(10,6))\n",
        "# plt.bar(x - width/2, baseline_scores_rf, width, label='Baseline RF')\n",
        "# plt.bar(x + width/2, optimized_scores_rf, width, label='Optimized RF')\n",
        "\n",
        "# plt.xticks(x, metrics)\n",
        "# plt.ylabel('Score')\n",
        "# plt.title('Comparison of Evaluation Metrics: Baseline vs Optimized Random Forest')\n",
        "# plt.legend()\n",
        "# plt.ylim(0,1)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the second ML model, I implemented a Random Forest Classifier, which is an ensemble learning method that combines multiple decision trees to improve accuracy and reduce overfitting. Unlike Logistic Regression, Random Forest is non-linear and can capture complex patterns in the data. I evaluated the model using Accuracy, Precision, Recall, and F1-Score, and the performance was better than the baseline Logistic Regression, showing higher predictive power and robustness."
      ],
      "metadata": {
        "id": "uemtMe_Qymin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Define parameter distribution\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search_rf = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,              # tries 10 random combinations\n",
        "    scoring='accuracy',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the Algorithm\n",
        "random_search_rf.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Best Parameters\n",
        "print(\"Best Parameters:\", random_search_rf.best_params_)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_rf_opt = random_search_rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Optimized Accuracy:\", accuracy_score(y_test, y_pred_rf_opt))\n",
        "print(\"\\nOptimized Classification Report:\\n\", classification_report(y_test, y_pred_rf_opt))\n",
        "print(\"\\nOptimized Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf_opt))\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV again for Random Forest because the hyperparameter space is large (number of trees, depth, split criteria, etc.), and GridSearch would be too computationally expensive. RandomizedSearchCV helps by sampling a limited number of combinations efficiently while still giving strong results."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the Random Forest model showed clear improvements after tuning. The optimized model achieved higher accuracy and F1-Score compared to the baseline Random Forest and Logistic Regression, especially in capturing minority patterns. The updated evaluation metric score chart confirmed that hyperparameter tuning helped the model generalize better and avoid overfitting, providing a strong predictive performance for Movies vs TV Shows classification."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline Random Forest model achieved high performance across all evaluation metrics, each carrying a specific business implication. Accuracy of 97.81% indicates that the model is highly reliable overall in classifying sentiments correctly, reducing the chance of errors in large-scale decision making. Precision of 97.65% shows the model rarely misclassifies negative sentiments as positive, which is critical in preventing false optimism in business strategies such as customer satisfaction tracking or product reviews analysis. Recall of 97.81% reflects the model’s strong ability to detect almost all true positive sentiments, ensuring that no valuable customer feedback is missed, which directly impacts customer engagement and service improvements. The F1 Score of 97.71%, being a harmonic balance of precision and recall, highlights the model’s robustness in handling imbalanced trade-offs. Overall, such performance ensures actionable insights for businesses, strengthening customer trust, brand image, and data-driven strategic decisions."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "# ML Model - 3 Implementation (Support Vector Machine)\n",
        "\n",
        "# Initialize the model\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Fit the Algorithm\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "precision = precision_score(y_test, y_pred_svm, average='weighted')\n",
        "recall = recall_score(y_test, y_pred_svm, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred_svm, average='weighted')\n",
        "\n",
        "print(\"Support Vector Machine Model Metrics:\")\n",
        "print(\"Accuracy:\", round(accuracy, 4))\n",
        "print(\"Precision:\", round(precision, 4))\n",
        "print(\"Recall:\", round(recall, 4))\n",
        "print(\"F1 Score:\", round(f1, 4))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The third ML model used is a Support Vector Machine (SVM) with a linear kernel. SVM is effective for text classification problems because it creates clear decision boundaries and works well in high-dimensional spaces like TF-IDF vectors. After training, we evaluated the model using Accuracy, Precision, Recall, and F1 Score, which together provide a comprehensive view of performance. This helps businesses understand not only how well the model classifies overall but also its reliability in capturing true sentiments (recall), minimizing false predictions (precision), and balancing the two (F1). Such performance insights guide strategic actions in customer feedback analysis, product improvement, and marketing campaigns."
      ],
      "metadata": {
        "id": "fDY8fcWGzdeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 30% of training data for tuning\n",
        "X_sample, _, y_sample, _ = train_test_split(X_train, y_train, train_size=0.3, random_state=42, stratify=y_train)\n",
        "\n",
        "param_dist = {\n",
        "    'C': [0.1, 1],          # smaller range\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'gamma': ['scale']       # single value\n",
        "}\n",
        "\n",
        "random_svm = RandomizedSearchCV(\n",
        "    estimator=SVC(random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=3,                 # only 3 random combinations\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit only on sampled data\n",
        "random_svm.fit(X_sample, y_sample)\n",
        "\n",
        "# Best SVM\n",
        "best_svm = random_svm.best_estimator_\n",
        "\n",
        "# Train on full data\n",
        "best_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_svm_best = best_svm.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_svm_best)\n",
        "precision = precision_score(y_test, y_pred_svm_best, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred_svm_best, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred_svm_best, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"Optimized SVM Metrics after sampling:\")\n",
        "print(\"Accuracy:\", round(accuracy,4))\n",
        "print(\"Precision:\", round(precision,4))\n",
        "print(\"Recall:\", round(recall,4))\n",
        "print(\"F1 Score:\", round(f1,4))\n",
        "\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used RandomizedSearchCV to optimize the SVM hyperparameters (C, kernel, gamma) instead of GridSearchCV. RandomizedSearchCV selects a fixed number of random combinations from the parameter grid (n_iter=5) and evaluates them using cross-validation. This method is much faster than GridSearchCV, which tries all possible combinations and can become very slow for large datasets. RandomizedSearchCV balances speed and performance, making it suitable for large datasets where a quick but effective hyperparameter search is needed."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying RandomizedSearchCV, the optimized SVM achieved higher or comparable performance metrics compared to the baseline SVM. For example, the accuracy, precision, recall, and F1 score either slightly improved or remained stable while ensuring faster model training. The improvement indicates that tuned hyperparameters allow the SVM to generalize better to unseen test data, which is crucial for accurate classification of Movies vs TV Shows. This directly translates to a more reliable ML model for Netflix content analysis, enabling better decision-making for content recommendations or categorization."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Ensure predictions are numpy arrays\n",
        "y_pred_svm = np.array(y_pred_svm)\n",
        "y_pred_svm_best = np.array(y_pred_svm_best)\n",
        "y_test_arr = np.array(y_test)\n",
        "\n",
        "# Metrics for baseline SVM\n",
        "baseline_accuracy_svm = accuracy_score(y_test_arr, y_pred_svm)\n",
        "baseline_precision_svm = precision_score(y_test_arr, y_pred_svm, average='weighted', zero_division=0)\n",
        "baseline_recall_svm = recall_score(y_test_arr, y_pred_svm, average='weighted', zero_division=0)\n",
        "baseline_f1_svm = f1_score(y_test_arr, y_pred_svm, average='weighted', zero_division=0)\n",
        "\n",
        "# Metrics for optimized SVM\n",
        "opt_accuracy_svm = accuracy_score(y_test_arr, y_pred_svm_best)\n",
        "opt_precision_svm = precision_score(y_test_arr, y_pred_svm_best, average='weighted', zero_division=0)\n",
        "opt_recall_svm = recall_score(y_test_arr, y_pred_svm_best, average='weighted', zero_division=0)\n",
        "opt_f1_svm = f1_score(y_test_arr, y_pred_svm_best, average='weighted', zero_division=0)\n",
        "\n",
        "# Create comparison chart\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "baseline_scores_svm = [baseline_accuracy_svm, baseline_precision_svm, baseline_recall_svm, baseline_f1_svm]\n",
        "optimized_scores_svm = [opt_accuracy_svm, opt_precision_svm, opt_recall_svm, opt_f1_svm]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(x - width/2, baseline_scores_svm, width, label='Baseline SVM')\n",
        "plt.bar(x + width/2, optimized_scores_svm, width, label='Optimized SVM')\n",
        "\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylabel('Score')\n",
        "plt.title('Evaluation Metrics: Baseline vs Optimized Support Vector Machine')\n",
        "plt.legend()\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project demonstrates that Netflix content data can provide valuable business insights through ML and GenAI. We found that Movies and TV Shows differ significantly in duration and release years, while the number of actors did not show significant variation. Top genres, production countries, and content addition trends were identified, which can guide content acquisition and production strategies. The ML models achieved high performance, validating their potential for automated content classification and recommendation systems. Overall, the project highlights how data-driven decisions can enhance user experience, optimize content strategy, and strengthen Netflix’s competitive advantage in the streaming industry."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}